{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:09:40.603558Z",
     "start_time": "2020-06-16T13:09:37.386688Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:32:07.954952Z",
     "start_time": "2020-06-16T14:32:07.949920Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Iris数据集进行理论演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:52:12.867786Z",
     "start_time": "2020-06-15T05:52:12.861225Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets #导入sklearn中的datasets模块，方便下载内置的iris数据集\n",
    "#导入数据\n",
    "x_data = datasets.load_iris().data #导入iris数据集的特征\n",
    "y_data = datasets.load_iris().target #导入iris数据集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T03:23:24.493947Z",
     "start_time": "2020-06-15T03:23:24.470784Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_data[:,0], x_data[:,2], c = y_data)\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:53:31.590446Z",
     "start_time": "2020-06-15T05:53:31.587199Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data = np.vstack([x_data[:,0],x_data[:,2]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1对1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:53:34.702961Z",
     "start_time": "2020-06-15T05:53:34.694976Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "one_vs_one_dataset = {}\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            idx1 = y_data == i\n",
    "            idx2 = y_data == j\n",
    "            x_data_i = x_data[idx1]\n",
    "            x_data_j = x_data[idx2]\n",
    "            y_data_i = y_data[idx1]\n",
    "            y_data_j = y_data[idx2]\n",
    "            y_data_i = np.zeros_like(y_data_i)\n",
    "            y_data_j = np.ones_like(y_data_j)\n",
    "            x_data_ = np.concatenate([x_data_i,x_data_j], axis=0)\n",
    "            y_data_ = np.concatenate([y_data_i,y_data_j], axis=0)\n",
    "            x_data_ = tf.cast(x_data_,dtype=tf.float32)\n",
    "            one_vs_one_dataset[id_] = (x_data_, y_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:53:36.228572Z",
     "start_time": "2020-06-15T05:53:36.222588Z"
    }
   },
   "outputs": [],
   "source": [
    "len(one_vs_one_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T04:36:52.249938Z",
     "start_time": "2020-06-15T04:36:52.242913Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def __call__(self, x, training=None):\n",
    "        logit = self.dense(x)\n",
    "        y_hat = tf.math.sigmoid(logit)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:55:07.356400Z",
     "start_time": "2020-06-15T05:55:07.347425Z"
    }
   },
   "outputs": [],
   "source": [
    "one_vs_one_lr = {}\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            one_vs_one_lr[id_] = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:55:12.485202Z",
     "start_time": "2020-06-15T05:55:07.586435Z"
    }
   },
   "outputs": [],
   "source": [
    "historys = []\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            lr_model = one_vs_one_lr[id_]\n",
    "            x, y  = one_vs_one_dataset[id_]\n",
    "            \n",
    "            x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "            y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "            lr_model.compile(optimizer='adam',\n",
    "                             loss='binary_crossentropy', # 这里用二元的交叉熵作为二分类的损失函数\n",
    "                             metrics=['acc'] # 在训练时输出accuracy(精度,即正确率)\n",
    "                            )\n",
    "            \n",
    "            historys.append(lr_model.fit(x, y, epochs=100, batch_size =16, verbose=0)) # verbose=0不从std输出,不然导出markdown这块就太长了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:55:12.581796Z",
     "start_time": "2020-06-15T05:55:12.515120Z"
    }
   },
   "outputs": [],
   "source": [
    "x_axis = range(100)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "for i in range(3):\n",
    "    loss = historys[i].history['loss']\n",
    "    ax[i].plot(x_axis, loss)\n",
    "    ax[i].set_title('classifier {} loss curve'.format(i+1))\n",
    "for i in range(3):\n",
    "    acc  = historys[i].history['acc']\n",
    "    ax[i+3].plot(x_axis, acc)\n",
    "    ax[i+3].set_title('classifier {} acc  curve'.format(i+1))\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[3].set_ylabel('acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:13:20.274278Z",
     "start_time": "2020-06-15T06:13:20.269134Z"
    }
   },
   "outputs": [],
   "source": [
    "ids_ = []\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            ids_.append(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:30:50.849485Z",
     "start_time": "2020-06-15T06:30:50.840005Z"
    }
   },
   "outputs": [],
   "source": [
    "id_ = ids_[2]\n",
    "lr_model = one_vs_one_lr[id_]\n",
    "x, y  = one_vs_one_dataset[id_]\n",
    "x_min, x_max = x.numpy()[:, 0].min() - 1, x.numpy()[:, 0].max() + 1\n",
    "y_min, y_max = x.numpy()[:, 1].min() - 1, x.numpy()[:, 1].max() + 1\n",
    "\n",
    "y_hat = x.numpy().dot(lr_model.weights[0].numpy()) + lr_model.weights[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:43:50.434672Z",
     "start_time": "2020-06-15T06:43:50.430397Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = [plt.cm.BuGn, plt.cm.BuPu, plt.cm.PuBuGn,\n",
    "         plt.cm.BuGn_r, plt.cm.BuPu_r, plt.cm.PuBuGn_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:47:55.113965Z",
     "start_time": "2020-06-15T06:47:55.106770Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary(id_):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    lr_model = one_vs_one_lr[id_]\n",
    "    x, y  = one_vs_one_dataset[id_]\n",
    "\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    z_hat = np.vstack([xx.ravel(), yy.ravel()]).T.dot(lr_model.weights[0].numpy()) + lr_model.weights[1].numpy()\n",
    "    z_hat = tf.math.sigmoid(z_hat.ravel())\n",
    "    z_hat = z_hat.numpy() > 0.5\n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=colors[i])\n",
    "    plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y, cmap=colors[i+3])\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('classifier {}'.format(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:01:29.828492Z",
     "start_time": "2020-06-15T07:01:29.784820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_boundary('0v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:01:34.893842Z",
     "start_time": "2020-06-15T07:01:34.862877Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary('0v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:48:36.965252Z",
     "start_time": "2020-06-15T06:48:36.919344Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary('1v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:17:59.921279Z",
     "start_time": "2020-06-15T07:17:59.917290Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:18:13.379261Z",
     "start_time": "2020-06-15T07:18:13.363754Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_multi_label(x_data, one_vs_one_lr):\n",
    "    votes = {i:np.zeros([x_data.shape[0],1]) for i in range(3)}\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            else:\n",
    "                id_ = str(i)+'v'+str(j)\n",
    "                lr_model = one_vs_one_lr[id_]\n",
    "                y_hat = x_data.dot(lr_model.weights[0].numpy()) + lr_model.weights[1].numpy()\n",
    "                y_hat = tf.math.sigmoid(y_hat)\n",
    "                votes[i] += (tf.cast(y_hat <= 0.5, dtype=tf.int32).numpy())\n",
    "                votes[j] += (tf.cast(y_hat > 0.5, dtype=tf.int32).numpy())\n",
    "    \n",
    "    votes = np.hstack([votes[0],votes[1],votes[2]])\n",
    "    y_hat = np.argmax(votes, axis=-1)\n",
    "    return y_hat\n",
    "                \n",
    "predict_multi_label(x_data, one_vs_one_lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:21:13.718949Z",
     "start_time": "2020-06-15T07:21:13.710619Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary_multi(one_vs_one_lr):\n",
    "       \n",
    "    plt.figure(figsize=(5,5))\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    z_hat = predict_multi_label(np.vstack([xx.ravel(), yy.ravel()]).T, one_vs_one_lr)  \n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn)\n",
    "    plt.scatter(x_data[:, 0], x_data[:, 1], c=y_data, cmap=plt.cm.YlOrBr)\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('multi classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:21:13.960674Z",
     "start_time": "2020-06-15T07:21:13.924746Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary_multi(one_vs_one_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:25:18.159777Z",
     "start_time": "2020-06-16T13:25:18.155770Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:24:12.505764Z",
     "start_time": "2020-06-15T07:24:12.494778Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_data, predict_multi_label(x_data, one_vs_one_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:24:29.628060Z",
     "start_time": "2020-06-15T07:24:29.623073Z"
    }
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一对多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:33:50.643805Z",
     "start_time": "2020-06-15T07:33:50.620683Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "one_vs_rest_dataset = {}\n",
    "for i in range(3):\n",
    "    id_ = str(i)\n",
    "    idx1 = y_data == i\n",
    "    idx2 = y_data != i\n",
    "    x_data_i = x_data[idx1]\n",
    "    x_data_j = x_data[idx2]\n",
    "    y_data_i = y_data[idx1]\n",
    "    y_data_j = y_data[idx2]\n",
    "    y_data_i = np.zeros_like(y_data_i)\n",
    "    y_data_j = np.ones_like(y_data_j)\n",
    "    x_data_ = np.concatenate([x_data_i,x_data_j], axis=0)\n",
    "    y_data_ = np.concatenate([y_data_i,y_data_j], axis=0)\n",
    "    x_data_ = tf.cast(x_data_,dtype=tf.float32)\n",
    "    one_vs_rest_dataset[id_] = (x_data_, y_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:39:13.541781Z",
     "start_time": "2020-06-15T07:39:13.535753Z"
    }
   },
   "outputs": [],
   "source": [
    "one_vs_rest_lr = {}\n",
    "for i in range(3):\n",
    "    id_ = str(i)\n",
    "    one_vs_rest_lr[id_] = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:39:57.069124Z",
     "start_time": "2020-06-15T07:39:27.372300Z"
    }
   },
   "outputs": [],
   "source": [
    "historys = []\n",
    "for i in range(3):\n",
    "    id_ = str(i)\n",
    "    lr_model = one_vs_rest_lr[id_]\n",
    "    x, y  = one_vs_rest_dataset[id_]\n",
    "\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    lr_model.compile(optimizer='adam',\n",
    "                     loss='binary_crossentropy', # 这里用二元的交叉熵作为二分类的损失函数\n",
    "                     metrics=['acc'] # 在训练时输出accuracy(精度,即正确率)\n",
    "                    )\n",
    "\n",
    "    historys.append(lr_model.fit(x, y, epochs=100, verbose=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:40:48.056192Z",
     "start_time": "2020-06-15T07:40:47.994290Z"
    }
   },
   "outputs": [],
   "source": [
    "x_axis = range(100)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "for i in range(3):\n",
    "    loss = historys[i].history['loss']\n",
    "    ax[i].plot(x_axis, loss)\n",
    "    ax[i].set_title('classifier {} loss curve'.format(i+1))\n",
    "for i in range(3):\n",
    "    acc  = historys[i].history['acc']\n",
    "    ax[i+3].plot(x_axis, acc)\n",
    "    ax[i+3].set_title('classifier {} acc  curve'.format(i+1))\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[3].set_ylabel('acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:48:06.026141Z",
     "start_time": "2020-06-15T07:48:06.021157Z"
    }
   },
   "outputs": [],
   "source": [
    "ids_ = []\n",
    "for i in range(3):\n",
    "    id_ = str(i)\n",
    "    ids_.append(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:49:48.160976Z",
     "start_time": "2020-06-15T07:49:48.151959Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary(id_):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    lr_model = one_vs_rest_lr[id_]\n",
    "    x, y  = one_vs_rest_dataset[id_]\n",
    "\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    z_hat = np.vstack([xx.ravel(), yy.ravel()]).T.dot(lr_model.weights[0].numpy()) + lr_model.weights[1].numpy()\n",
    "    z_hat = tf.math.sigmoid(z_hat.ravel())\n",
    "    z_hat = z_hat.numpy() > 0.5\n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=colors[i])\n",
    "    plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y, cmap=colors[i+3])\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('classifier {} vs rest'.format(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T07:51:18.344397Z",
     "start_time": "2020-06-15T07:51:18.301004Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary(ids_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T08:01:16.709181Z",
     "start_time": "2020-06-15T08:01:16.676270Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_multi_label(x_data, one_vs_rest_lr):\n",
    "    votes = {i:np.zeros([x_data.shape[0],1]) for i in range(3)}\n",
    "    for i in range(3):\n",
    "        id_ = str(i)\n",
    "        lr_model = one_vs_rest_lr[id_]\n",
    "        y_hat = x_data.dot(lr_model.weights[0].numpy()) + lr_model.weights[1].numpy()\n",
    "        y_hat = tf.math.sigmoid(y_hat)\n",
    "        votes[i] +=  (1 - y_hat)\n",
    "\n",
    "    votes = np.hstack([votes[0],votes[1],votes[2]])\n",
    "    y_hat = np.argmax(votes, axis=-1)\n",
    "    return y_hat\n",
    "                \n",
    "predict_multi_label(x_data, one_vs_rest_lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T08:01:20.789796Z",
     "start_time": "2020-06-15T08:01:20.781494Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary_multi(one_vs_rest_lr):\n",
    "       \n",
    "    plt.figure(figsize=(5,5))\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    z_hat = predict_multi_label(np.vstack([xx.ravel(), yy.ravel()]).T, one_vs_rest_lr)  \n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn)\n",
    "    plt.scatter(x_data[:, 0], x_data[:, 1], c=y_data, cmap=plt.cm.YlOrBr)\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('multi classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T08:01:21.648965Z",
     "start_time": "2020-06-15T08:01:21.616036Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary_multi(one_vs_rest_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T08:08:45.357683Z",
     "start_time": "2020-06-15T08:08:45.330736Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_data, predict_multi_label(x_data, one_vs_rest_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T08:08:48.279439Z",
     "start_time": "2020-06-15T08:08:48.275450Z"
    }
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:34:55.712230Z",
     "start_time": "2020-06-15T10:34:55.694239Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionPlus(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_of_labels):\n",
    "        super(LogisticRegressionPlus, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(num_of_labels)\n",
    "        \n",
    "    def __call__(self, x, training=None):\n",
    "        logit = self.dense(x)\n",
    "        y_hat = tf.math.softmax(logit)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:34:58.856162Z",
     "start_time": "2020-06-15T10:34:58.853147Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_plus = LogisticRegressionPlus(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:35:01.915002Z",
     "start_time": "2020-06-15T10:35:01.910954Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_plus.compile(optimizer='adam',loss='sparse_categorical_crossentropy', # 这里用二元的交叉熵作为二分类的损失函数\n",
    "                metrics=['acc'] # 在训练时输出accuracy(精度,即正确率)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:38:05.570884Z",
     "start_time": "2020-06-15T10:37:50.880210Z"
    }
   },
   "outputs": [],
   "source": [
    "history = LR_plus.fit(x_data, y_data, epochs=100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:43:44.528950Z",
     "start_time": "2020-06-15T10:43:44.498020Z"
    }
   },
   "outputs": [],
   "source": [
    "x_axis = range(100)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "loss = history.history['loss']\n",
    "ax[0].plot(x_axis, loss)\n",
    "ax[0].set_title('softmax classifier loss curve')\n",
    "\n",
    "acc  = history.history['acc']\n",
    "ax[1].plot(x_axis, acc)\n",
    "ax[1].set_title('softmax classifier acc  curve')\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[1].set_ylabel('acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:40:39.690376Z",
     "start_time": "2020-06-15T10:40:39.684093Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary_multi():\n",
    "       \n",
    "    plt.figure(figsize=(5,5))\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    z_hat = np.argmax(LR_plus(np.vstack([xx.ravel(), yy.ravel()]).T).numpy(),axis=-1)\n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn)\n",
    "    plt.scatter(x_data[:, 0], x_data[:, 1], c=y_data, cmap=plt.cm.YlOrBr)\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('softmax classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:40:40.038369Z",
     "start_time": "2020-06-15T10:40:40.007452Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T10:52:08.295569Z",
     "start_time": "2020-06-15T10:52:08.271588Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_data, np.argmax(LR_plus(x_data).numpy(),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:02:47.824174Z",
     "start_time": "2020-06-15T15:02:47.802219Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_plus.weights[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:18:27.345023Z",
     "start_time": "2020-06-15T15:18:27.337043Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary(id_):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    x, y  = one_vs_one_dataset[id_]\n",
    "    i,j = map(int, id_.split('v'))\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    w = LR_plus.weights[0].numpy()[:,i] - LR_plus.weights[0].numpy()[:,j]\n",
    "    b = LR_plus.weights[1].numpy()[i] - LR_plus.weights[1].numpy()[j]\n",
    "    z_hat = np.vstack([xx.ravel(), yy.ravel()]).T.dot(w) + b\n",
    "    \n",
    "    z_hat = tf.math.sigmoid(z_hat.ravel())\n",
    "    z_hat = z_hat.numpy() < 0.5\n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=colors[0])\n",
    "    plt.scatter(x.numpy()[:, 0], x.numpy()[:, 1], c=y, cmap=colors[3])\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('classifier softmax {}'.format(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:18:57.819561Z",
     "start_time": "2020-06-15T15:18:57.786995Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary('0v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:16:02.559806Z",
     "start_time": "2020-06-15T15:16:02.524537Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_multi_label(x_data):\n",
    "    votes = {i:np.zeros([x_data.shape[0],]) for i in range(3)}\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            else:\n",
    "                w = LR_plus.weights[0].numpy()[:,i] - LR_plus.weights[0].numpy()[:,j]\n",
    "                b = LR_plus.weights[1].numpy()[i] - LR_plus.weights[1].numpy()[j]\n",
    "                \n",
    "                y_hat = x_data.dot(w) + b\n",
    "                y_hat = tf.math.sigmoid(y_hat)\n",
    "                # print(y_hat)\n",
    "                \n",
    "                votes[i] += (tf.cast(y_hat > 0.5, dtype=tf.int32).numpy())\n",
    "                votes[j] += (tf.cast(y_hat <= 0.5, dtype=tf.int32).numpy())\n",
    "    \n",
    "    votes = np.vstack([votes[0],votes[1],votes[2]])\n",
    "    # print(votes)\n",
    "    y_hat = np.argmax(votes, axis=0)\n",
    "    return y_hat\n",
    "                \n",
    "predict_multi_label(x_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:16:27.410551Z",
     "start_time": "2020-06-15T15:16:27.401575Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boundary_multi():\n",
    "       \n",
    "    plt.figure(figsize=(5,5))\n",
    "    x_min, x_max = 0, 8\n",
    "    y_min, y_max = 0, 10\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    z_hat = predict_multi_label(np.vstack([xx.ravel(), yy.ravel()]).T)  \n",
    "    Z = z_hat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlGn)\n",
    "    plt.scatter(x_data[:, 0], x_data[:, 1], c=y_data, cmap=plt.cm.YlOrBr)\n",
    "\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Petal length')\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.title('softmax 1v1 classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:16:28.947703Z",
     "start_time": "2020-06-15T15:16:28.904815Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_boundary_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:31:11.524678Z",
     "start_time": "2020-06-15T15:31:11.494820Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_data, predict_multi_label(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:09:47.428579Z",
     "start_time": "2020-06-16T13:09:47.341904Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load('TibetanMNIST.npz')\n",
    "X = data['image']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:40:03.985328Z",
     "start_time": "2020-06-15T15:40:03.757715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=5,\n",
    "    sharex=True,\n",
    "    sharey=True, )\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Blues_r', interpolation='nearest')\n",
    "    ax[i].set_title(str(i))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:40:06.915630Z",
     "start_time": "2020-06-15T15:40:06.365030Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = []\n",
    "for i in range(10):\n",
    "    count.append(sum(y == i))\n",
    "\n",
    "f, ax1 = plt.subplots(1, 1, figsize=(7, 3), sharex=True)\n",
    "\n",
    "x = np.array(list(\"0123456789\"))\n",
    "y1 = np.array(count)\n",
    "sns.barplot(x=x, y=y1, palette=\"rocket\", ax=ax1)\n",
    "ax1.axhline(0, color=\"k\", clip_on=False)\n",
    "ax1.set_ylabel(\"frequency of occurrence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:09:50.838554Z",
     "start_time": "2020-06-16T13:09:50.467392Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [xx.reshape(28*28) for xx in X]\n",
    "\n",
    "x_train,x_test,y_data,y_data_test = train_test_split(x,y,test_size=0.3,train_size=0.7,random_state=0)\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train)  # 计算均值和方差\n",
    "x_data = sc.transform(x_train) #利用计算好的方差和均值进行Z分数标准化\n",
    "x_data_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:10:12.962790Z",
     "start_time": "2020-06-16T13:10:11.245551Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "one_vs_one_dataset = {}\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            idx1 = y_data == i\n",
    "            idx2 = y_data == j\n",
    "            x_data_i = x_data[idx1]\n",
    "            x_data_j = x_data[idx2]\n",
    "            y_data_i = y_data[idx1]\n",
    "            y_data_j = y_data[idx2]\n",
    "            y_data_i = np.zeros_like(y_data_i)\n",
    "            y_data_j = np.ones_like(y_data_j)\n",
    "            x_data_ = np.concatenate([x_data_i,x_data_j], axis=0)\n",
    "            y_data_ = np.concatenate([y_data_i,y_data_j], axis=0)\n",
    "            x_data_ = tf.cast(x_data_,dtype=tf.float32)\n",
    "            one_vs_one_dataset[id_] = (x_data_, y_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:10:18.341018Z",
     "start_time": "2020-06-16T13:10:18.336508Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def __call__(self, x, training=None):\n",
    "        logit = self.dense(x)\n",
    "        y_hat = tf.math.sigmoid(logit)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:10:39.446550Z",
     "start_time": "2020-06-16T13:10:39.377589Z"
    }
   },
   "outputs": [],
   "source": [
    "one_vs_one_lr = {}\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i == j:\n",
    "            continue\n",
    "        else:\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            one_vs_one_lr[id_] = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:16:28.013032Z",
     "start_time": "2020-06-16T13:13:40.124009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "historys = []\n",
    "times = []\n",
    "accs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            start_ = time.time()\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            print('在训练',id_)\n",
    "            lr_model = one_vs_one_lr[id_]\n",
    "            x, y  = one_vs_one_dataset[id_]\n",
    "            \n",
    "            x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "            y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "            lr_model.compile(optimizer='adam',\n",
    "                             loss='binary_crossentropy', # 这里用二元的交叉熵作为二分类的损失函数\n",
    "                             metrics=['acc'] # 在训练时输出accuracy(精度,即正确率)\n",
    "                            )\n",
    "            \n",
    "            historys.append(lr_model.fit(x, y, epochs=10, batch_size =16, verbose=0)) \n",
    "            times.append(time.time()-start_)\n",
    "            accs.append(sum(lr_model.predict(x_data_test) == y_data_test)/len(y_data_test))\n",
    "            # verbose=0不从std输出,不然导出markdown这块就太长了\n",
    "\n",
    "\n",
    "time_count = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:16:36.431384Z",
     "start_time": "2020-06-16T13:16:36.415551Z"
    }
   },
   "outputs": [],
   "source": [
    "time_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:53:49.980389Z",
     "start_time": "2020-06-16T13:53:49.975402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:16:41.476701Z",
     "start_time": "2020-06-16T13:16:41.471706Z"
    }
   },
   "outputs": [],
   "source": [
    "time_count/45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:20:04.657767Z",
     "start_time": "2020-06-16T13:20:00.522072Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_axis = range(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,36), nrows=9, ncols=5, sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "for i in range(45):\n",
    "    acc = historys[i].history['acc']\n",
    "    ax[i].plot(x_axis, acc)\n",
    "    ax[i].set_title('clf {} acc'.format(i+1))\n",
    "\n",
    "plt.savefig('jinx.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:20:47.648751Z",
     "start_time": "2020-06-16T13:20:43.622097Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_axis = range(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,36), nrows=9, ncols=5, sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "for i in range(45):\n",
    "    loss = historys[i].history['loss']\n",
    "    ax[i].plot(x_axis, loss)\n",
    "    ax[i].set_title('clf {} loss'.format(i+1))\n",
    "\n",
    "plt.savefig('jinx1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:23:19.069177Z",
     "start_time": "2020-06-16T13:23:17.915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            id_ = str(i)+'v'+str(j)\n",
    "            print('在测试',id_)\n",
    "            lr_model = one_vs_one_lr[id_]\n",
    "\n",
    "            accs.append(sum(lr_model.predict(x_data_test) == y_data_test)/len(y_data_test))\n",
    "            # verbose=0不从std输出,不然导出markdown这块就太长了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:26:43.362229Z",
     "start_time": "2020-06-16T13:26:43.353253Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_multi_label(x_data, one_vs_one_lr):\n",
    "    votes = {i:np.zeros([x_data.shape[0],1]) for i in range(10)}\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            else:\n",
    "                id_ = str(i)+'v'+str(j)\n",
    "                lr_model = one_vs_one_lr[id_]\n",
    "                y_hat = x_data.dot(lr_model.weights[0].numpy()) + lr_model.weights[1].numpy()\n",
    "                y_hat = tf.math.sigmoid(y_hat)\n",
    "                votes[i] += (tf.cast(y_hat <= 0.5, dtype=tf.int32).numpy())\n",
    "                votes[j] += (tf.cast(y_hat > 0.5, dtype=tf.int32).numpy())\n",
    "    \n",
    "    votes = np.hstack([votes[0],votes[1],votes[2],votes[3],votes[4],votes[5],votes[6],votes[7],votes[8],votes[9]])\n",
    "    y_hat = np.argmax(votes, axis=-1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:13:34.827772Z",
     "start_time": "2020-06-16T14:13:34.509289Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_data, predict_multi_label(x_data, one_vs_one_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:14:06.974499Z",
     "start_time": "2020-06-16T14:14:06.969513Z"
    }
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:21:46.577731Z",
     "start_time": "2020-06-16T14:21:46.573250Z"
    }
   },
   "outputs": [],
   "source": [
    "12097/12476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:22:17.687048Z",
     "start_time": "2020-06-16T14:22:17.683059Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionPlus(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_of_labels):\n",
    "        super(LogisticRegressionPlus, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(num_of_labels)\n",
    "        \n",
    "    def __call__(self, x, training=None):\n",
    "        logit = self.dense(x)\n",
    "        y_hat = tf.math.softmax(logit)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:24:22.825361Z",
     "start_time": "2020-06-16T14:24:22.820346Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_plus = LogisticRegressionPlus(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:24:25.380276Z",
     "start_time": "2020-06-16T14:24:25.375246Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_plus.compile(optimizer='adam',loss='sparse_categorical_crossentropy', # 这里用二元的交叉熵作为二分类的损失函数\n",
    "                metrics=['acc'] # 在训练时输出accuracy(精度,即正确率)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:25:03.634170Z",
     "start_time": "2020-06-16T14:24:30.141877Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "history = LR_plus.fit(x_data, y_data, batch_size=16, epochs=10, verbose =1)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:25:45.899008Z",
     "start_time": "2020-06-16T14:25:45.840569Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_data, np.argmax(LR_plus(x_data).numpy(),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:25:50.360788Z",
     "start_time": "2020-06-16T14:25:50.355804Z"
    }
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:32:32.025674Z",
     "start_time": "2020-06-16T14:32:31.802272Z"
    }
   },
   "outputs": [],
   "source": [
    "x_axis = range(10)\n",
    "fig, ax = plt.subplots(figsize=(12,6), nrows=1, ncols=2, sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "loss = history.history['loss']\n",
    "ax[0].plot(x_axis, loss)\n",
    "ax[0].set_title('softmax classifier loss curve')\n",
    "\n",
    "acc  = history.history['acc']\n",
    "ax[1].plot(x_axis, acc)\n",
    "ax[1].set_title('softmax classifier acc  curve')\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[1].set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:40:21.032025Z",
     "start_time": "2020-06-16T14:40:21.028102Z"
    }
   },
   "outputs": [],
   "source": [
    "12214/12476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
